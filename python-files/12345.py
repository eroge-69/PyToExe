"""
NeuroSparse Transformer - –ë–∏–æ-–∏–Ω—Å–ø–∏—Ä–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —ç–Ω–µ—Ä–≥–æ—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞
–°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞–º: PEP-8, SOLID, DRY
–í–µ—Ä—Å–∏—è: 1.2
"""

import os
import json
import random
from datetime import datetime
import torch
import torch.nn as nn
import torch.optim as optim
from transformers import AutoTokenizer
from PyQt5.QtWidgets import (QApplication, QMainWindow, QTextEdit, QPushButton,
                            QVBoxLayout, QHBoxLayout, QWidget, QLabel, QProgressBar,
                            QFileDialog, QSlider, QSpinBox, QAction)
from PyQt5.QtCore import QThread, pyqtSignal, Qt
from PyQt5.QtGui import QFont

# --- –Ø–¥—Ä–æ –º–æ–¥–µ–ª–∏ ---
class DynamicSparsityGate(nn.Module):
    """–î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π –≥–µ–π—Ç —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ—Å—Ç–∏ (–∞–Ω–∞–ª–æ–≥ –Ω–µ–π—Ä–æ–º–æ–¥—É–ª—è—Ç–æ—Ä–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã)"""
    def __init__(self, dim: int, init_sparsity: float = 0.15):
        super().__init__()
        self.context_gate = nn.Linear(dim, 1, bias=False)
        self.sparsity_level = nn.Parameter(torch.tensor(init_sparsity))
       
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –±–∏–Ω–∞—Ä–Ω–æ–π –º–∞—Å–∫–∏ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏"""
        scores = self.context_gate(x).squeeze(-1)
        k = max(1, int(x.size(1) * torch.sigmoid(self.sparsity_level)))
        threshold = torch.topk(scores, k, dim=-1).values[:, -1:]
        return (scores > threshold).float()

class AstroNorm(nn.Module):
    """–ê—Å—Ç—Ä–æ—Ü–∏—Ç–∞—Ä–Ω–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è (—Ä–µ–≥—É–ª—è—Ü–∏—è —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π)"""
    def __init__(self, dim: int, decay: float = 0.95):
        super().__init__()
        self.buffer = nn.Parameter(torch.zeros(dim))
        self.decay = nn.Parameter(torch.tensor(decay))
       
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """–°—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏—è –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö —Å –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–º –±—É—Ñ–µ—Ä–æ–º"""
        mean = x.detach().mean(dim=1, keepdim=True)
        var = x.detach().var(dim=1, keepdim=True)
       
        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –±—É—Ñ–µ—Ä–∞ —Å —ç–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–º –∑–∞—Ç—É—Ö–∞–Ω–∏–µ–º
        self.buffer.data = self.decay * self.buffer + (1 - self.decay) * mean.squeeze()
       
        # –ü–æ–¥–∞–≤–ª–µ–Ω–∏–µ —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
        buffer_mask = (x - mean).abs() > self.buffer.abs()
        return torch.where(buffer_mask, mean, x) / (var + 1e-6).sqrt()

class SparseExpert(nn.Module):
    """–≠–∫—Å–ø–µ—Ä—Ç–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ —Å –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–π —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ—Å—Ç—å—é"""
    def __init__(self, dim: int, num_experts: int = 4):
        super().__init__()
        self.experts = nn.ModuleList([nn.Linear(dim, dim) for _ in range(num_experts)])
        self.gating = nn.Linear(dim, num_experts)
        self.sparsity_constraint = nn.Parameter(torch.tensor(0.8))
       
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """–ê–∫—Ç–∏–≤–∞—Ü–∏—è —Ç–æ–ª—å–∫–æ –Ω–∞–∏–±–æ–ª–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —ç–∫—Å–ø–µ—Ä—Ç–æ–≤"""
        gate_scores = self.gating(x)
        k = max(1, int(self.sparsity_constraint * len(self.experts)))
        topk_idx = torch.topk(gate_scores, k, dim=-1).indices
       
        output = torch.zeros_like(x)
        for i, expert in enumerate(self.experts):
            expert_mask = (topk_idx == i).any(dim=-1)
            output[expert_mask] += expert(x[expert_mask])
           
        return output

class NeuroSparseTransformer(nn.Module):
    """–û—Å–Ω–æ–≤–Ω–∞—è –º–æ–¥–µ–ª—å NeuroSparse Transformer"""
    def __init__(self, vocab_size: int, dim: int = 128, num_layers: int = 4, num_ensembles: int = 4):
        super().__init__()
        self.dim = dim
        self.embedding = nn.Embedding(vocab_size, dim)
        self.layers = nn.ModuleList([self._build_layer(dim, num_ensembles) for _ in range(num_layers)])
        self.norm = nn.LayerNorm(dim)
        self.head = nn.Linear(dim, vocab_size)
       
    def _build_layer(self, dim: int, num_ensembles: int) -> nn.ModuleDict:
        """–ö–æ–Ω—Å—Ç—Ä—É–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–ª–æ—è —Å –±–∏–æ-–∏–Ω—Å–ø–∏—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞–º–∏"""
        return nn.ModuleDict({
            'attn_norm': AstroNorm(dim),
            'sparse_gate': DynamicSparsityGate(dim),
            'expert_ffn': SparseExpert(dim, num_experts=num_ensembles)
        })
       
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """–ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥ —Å —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–π –∞–∫—Ç–∏–≤–∞—Ü–∏–µ–π"""
        x = self.embedding(x)
       
        for layer in self.layers:
            # –ê—Å—Ç—Ä–æ—Ü–∏—Ç–∞—Ä–Ω–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
            x = layer['attn_norm'](x)
           
            # –ù–µ–π—Ä–æ–º–æ–¥—É–ª—è—Ç–æ—Ä–Ω–æ–µ –≥–µ–π—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
            mask = layer['sparse_gate'](x)
            x = x * mask.unsqueeze(-1)
           
            # –≠–∫—Å–ø–µ—Ä—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞
            x = layer['expert_ffn'](x)
           
        return self.head(self.norm(x))

# --- –ú–µ—Ö–∞–Ω–∏–∑–º—ã –æ–±—É—á–µ–Ω–∏—è ---
def neuro_consolidation(model: nn.Module):
    """–ü—Ä–æ—Ü–µ–¥—É—Ä–∞ –∫–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏–∏ –∑–Ω–∞–Ω–∏–π —á–µ—Ä–µ–∑ '—Å–æ–Ω'"""
    model.eval()
    for module in model.modules():
        if isinstance(module, SparseExpert):
            # –°—Ç–æ—Ö–∞—Å—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–∫–æ–º–±–∏–Ω–∞—Ü–∏—è —ç–∫—Å–ø–µ—Ä—Ç–æ–≤
            for i in range(len(module.experts)):
                j = random.choice([idx for idx in range(len(module.experts)) if idx != i])
                alpha = random.uniform(0.3, 0.7)
               
                with torch.no_grad():
                    for param_i, param_j in zip(module.experts[i].parameters(),
                                                module.experts[j].parameters()):
                        # –ü–µ—Ä–µ–∫—Ä–µ—Å—Ç–Ω–æ–µ —Å–º–µ—à–∏–≤–∞–Ω–∏–µ –≤–µ—Å–æ–≤
                        mix = alpha * param_i + (1 - alpha) * param_j
                        param_i.copy_(mix)
                        param_j.copy_(mix)

class TextProcessor:
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ —Ç–µ–∫—Å—Ç–∞ —Å —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–µ–π"""
    def __init__(self):
        self.tokenizer = AutoTokenizer.from_pretrained("gpt2")
        self.tokenizer.add_special_tokens({'pad_token': '[PAD]'})
       
    @property
    def vocab_size(self) -> int:
        return len(self.tokenizer)
   
    def encode(self, text: str) -> torch.Tensor:
        return self.tokenizer.encode(text, return_tensors="pt",
                                    max_length=256, truncation=True, padding=True)
   
    def decode(self, tokens: torch.Tensor) -> str:
        return self.tokenizer.decode(tokens.squeeze().tolist(), skip_special_tokens=True)

# --- GUI –∏ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏ ---
class TrainingThread(QThread):
    """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –ø–æ—Ç–æ–∫ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏"""
    progress_updated = pyqtSignal(int)
    training_completed = pyqtSignal(str)
    status_updated = pyqtSignal(str)

    def __init__(self, model: nn.Module, processor: TextProcessor, text_data: str):
        super().__init__()
        self.model = model
        self.processor = processor
        self.text_data = text_data
        self.is_running = True

    def run(self):
        try:
            inputs = self.processor.encode(self.text_data)
            dataset = torch.utils.data.TensorDataset(inputs)
            loader = torch.utils.data.DataLoader(dataset, batch_size=2, shuffle=True)
           
            optimizer = optim.AdamW(self.model.parameters(), lr=1e-4)
            criterion = nn.CrossEntropyLoss(ignore_index=self.processor.tokenizer.pad_token_id)
           
            total_epochs = 10
            for epoch in range(total_epochs):
                if not self.is_running:
                    return
               
                self.status_updated.emit(f"–≠–ø–æ—Ö–∞ {epoch+1}/{total_epochs}")
                epoch_loss = 0.0
               
                for batch in loader:
                    optimizer.zero_grad()
                    input_ids = batch[0]
                   
                    # –£—Å–µ—á–µ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏
                    outputs = self.model(input_ids[:, :-1])
                    loss = criterion(outputs.view(-1, self.model.head.out_features),
                                    input_ids[:, 1:].contiguous().view(-1))
                   
                    loss.backward()
                    optimizer.step()
                    epoch_loss += loss.item()
               
                # –ö–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏—è –∫–∞–∂–¥—ã–µ 2 —ç–ø–æ—Ö–∏
                if epoch % 2 == 0:
                    self.status_updated.emit("–ö–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏—è –∑–Ω–∞–Ω–∏–π...")
                    neuro_consolidation(self.model)
               
                self.progress_updated.emit(int((epoch+1) / total_epochs * 100))
           
            self.training_completed.emit(f"–û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ | Loss: {epoch_loss/len(loader):.4f}")
        except Exception as e:
            self.training_completed.emit(f"–û—à–∏–±–∫–∞: {str(e)}")

    def stop(self):
        self.is_running = False

class NeuroSparseApp(QMainWindow):
    """–ì–ª–∞–≤–Ω–æ–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ —Å –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–º –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–º"""
    def __init__(self):
        super().__init__()
        self.model = None
        self.processor = TextProcessor()
        self.training_thread = None
        self._init_ui()
       
    def _init_ui(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞"""
        self.setWindowTitle("NeuroSparse Transformer")
        self.setGeometry(100, 100, 1000, 700)
       
        # –¶–µ–Ω—Ç—Ä–∞–ª—å–Ω—ã–π –≤–∏–¥–∂–µ—Ç –∏ –º–∞–∫–µ—Ç
        central_widget = QWidget()
        main_layout = QHBoxLayout(central_widget)
       
        # –õ–µ–≤–∞—è –ø–∞–Ω–µ–ª—å (–û–±—É—á–µ–Ω–∏–µ)
        train_panel = QVBoxLayout()
        train_panel.addWidget(QLabel("–î–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è:"))
       
        self.train_input = QTextEdit()
        self.train_input.setPlaceholderText("–í–≤–µ–¥–∏—Ç–µ —Ç–µ–∫—Å—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è...")
        self.train_input.setFont(QFont("Consolas", 10))
        train_panel.addWidget(self.train_input)
       
        self.train_btn = QPushButton("–û–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å")
        self.train_btn.clicked.connect(self.start_training)
        train_panel.addWidget(self.train_btn)
       
        self.progress_bar = QProgressBar()
        train_panel.addWidget(self.progress_bar)
       
        self.status_label = QLabel("–ì–æ—Ç–æ–≤ –∫ –æ–±—É—á–µ–Ω–∏—é")
        train_panel.addWidget(self.status_label)
       
        # –ü—Ä–∞–≤–∞—è –ø–∞–Ω–µ–ª—å (–í–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ)
        interact_panel = QVBoxLayout()
        interact_panel.addWidget(QLabel("–í–æ–ø—Ä–æ—Å:"))
       
        self.question_input = QTextEdit()
        self.question_input.setPlaceholderText("–í–≤–µ–¥–∏—Ç–µ –≤–∞—à –≤–æ–ø—Ä–æ—Å...")
        self.question_input.setFont(QFont("Consolas", 10))
        interact_panel.addWidget(self.question_input)
       
        # –ü–∞–Ω–µ–ª—å –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        param_layout = QHBoxLayout()
        param_layout.addWidget(QLabel("–¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞:"))
       
        self.temp_slider = QSlider(Qt.Horizontal)
        self.temp_slider.setRange(10, 100)
        self.temp_slider.setValue(70)
        param_layout.addWidget(self.temp_slider)
       
        param_layout.addWidget(QLabel("Top-K:"))
        self.topk_spin = QSpinBox()
        self.topk_spin.setRange(1, 500)
        self.topk_spin.setValue(50)
        param_layout.addWidget(self.topk_spin)
       
        interact_panel.addLayout(param_layout)
       
        self.ask_btn = QPushButton("–ó–∞–¥–∞—Ç—å –≤–æ–ø—Ä–æ—Å")
        self.ask_btn.clicked.connect(self.ask_question)
        interact_panel.addWidget(self.ask_btn)
       
        interact_panel.addWidget(QLabel("–û—Ç–≤–µ—Ç:"))
        self.answer_output = QTextEdit()
        self.answer_output.setReadOnly(True)
        self.answer_output.setFont(QFont("Consolas", 10))
        interact_panel.addWidget(self.answer_output)
       
        # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –ø–∞–Ω–µ–ª–µ–π
        main_layout.addLayout(train_panel, 50)
        main_layout.addLayout(interact_panel, 50)
       
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –≥–ª–∞–≤–Ω–æ–≥–æ –æ–∫–Ω–∞
        self.setCentralWidget(central_widget)
        self._create_toolbar()
        self._create_statusbar()
       
    def _create_toolbar(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –ø–∞–Ω–µ–ª–∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤"""
        toolbar = self.addToolBar("Tools")
       
        # –î–µ–π—Å—Ç–≤–∏—è –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤
        self.save_action = QAction("üíæ –°–æ—Ö—Ä–∞–Ω–∏—Ç—å", self)
        self.save_action.triggered.connect(self.save_model)
       
        self.load_action = QAction("üìÇ –ó–∞–≥—Ä—É–∑–∏—Ç—å", self)
        self.load_action.triggered.connect(self.load_model)
       
        self.consolidate_action = QAction("üåô –ö–æ–Ω—Å–æ–ª–∏–¥–∏—Ä–æ–≤–∞—Ç—å", self)
        self.consolidate_action.triggered.connect(self.consolidate_model)
       
        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –≤ –ø–∞–Ω–µ–ª—å
        toolbar.addAction(self.save_action)
        toolbar.addAction(self.load_action)
        toolbar.addAction(self.consolidate_action)
       
    def _create_statusbar(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ —Å—Ç—Ä–æ–∫–∏ —Å–æ—Å—Ç–æ—è–Ω–∏—è"""
        self.statusBar().showMessage("–°–∏—Å—Ç–µ–º–∞ –≥–æ—Ç–æ–≤–∞")
       
    def start_training(self):
        """–ó–∞–ø—É—Å–∫ –ø—Ä–æ—Ü–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è"""
        if self.training_thread and self.training_thread.isRunning():
            return
           
        text_data = self.train_input.toPlainText()
        if len(text_data) < 500:
            self.status_label.setText("–¢—Ä–µ–±—É–µ—Ç—Å—è –º–∏–Ω–∏–º—É–º 500 —Å–∏–º–≤–æ–ª–æ–≤")
            return
           
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏
        self.model = NeuroSparseTransformer(self.processor.vocab_size)
       
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∏ –∑–∞–ø—É—Å–∫ –ø–æ—Ç–æ–∫–∞ –æ–±—É—á–µ–Ω–∏—è
        self.training_thread = TrainingThread(self.model, self.processor, text_data)
        self.training_thread.progress_updated.connect(self.progress_bar.setValue)
        self.training_thread.training_completed.connect(self.status_label.setText)
        self.training_thread.status_updated.connect(self.statusBar().showMessage)
        self.training_thread.start()
       
        # –ë–ª–æ–∫–∏—Ä–æ–≤–∫–∞ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞ –Ω–∞ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è
        self.train_btn.setEnabled(False)
        self.training_thread.finished.connect(lambda: self.train_btn.setEnabled(True))
       
    def ask_question(self):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–æ–ø—Ä–æ—Å"""
        if not self.model:
            self.answer_output.setText("–°–Ω–∞—á–∞–ª–∞ –æ–±—É—á–∏—Ç–µ –º–æ–¥–µ–ª—å!")
            return
           
        question = self.question_input.toPlainText().strip()
        if not question:
            self.answer_output.setText("–í–≤–µ–¥–∏—Ç–µ –≤–æ–ø—Ä–æ—Å")
            return
           
        try:
            # –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤–æ–ø—Ä–æ—Å–∞
            input_ids = self.processor.encode(question)
           
            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞
            output_ids = self._generate_text(
                input_ids,
                max_length=100,
                temperature=self.temp_slider.value() / 100,
                top_k=self.topk_spin.value()
            )
           
            # –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –≤—ã–≤–æ–¥
            answer = self.processor.decode(output_ids)
            self.answer_output.setText(f"–û—Ç–≤–µ—Ç: {answer}")
        except Exception as e:
            self.answer_output.setText(f"–û—à–∏–±–∫–∞: {str(e)}")
           
    def _generate_text(self, input_ids: torch.Tensor, max_length: int = 100,
                      temperature: float = 0.7, top_k: int = 50) -> torch.Tensor:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ —Å –∑–∞–¥–∞–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏"""
        self.model.eval()
        generated = input_ids.clone()
       
        with torch.no_grad():
            for _ in range(max_length):
                outputs = self.model(generated)
                logits = outputs[:, -1, :] / temperature
               
                # Top-k —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è
                values, indices = torch.topk(logits, top_k)
                probs = torch.softmax(values, dim=-1)
               
                # –°—Ç–æ—Ö–∞—Å—Ç–∏—á–µ—Å–∫–∞—è –≤—ã–±–æ—Ä–∫–∞
                next_token = indices[0, torch.multinomial(probs[0], 1)]
                generated = torch.cat([generated, next_token.unsqueeze(0)], dim=-1)
               
                # –ö—Ä–∏—Ç–µ—Ä–∏–π –æ—Å—Ç–∞–Ω–æ–≤–∫–∏
                if next_token.item() == self.processor.tokenizer.eos_token_id:
                    break
                   
        return generated
   
    def save_model(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ –¥–∏—Å–∫"""
        if not self.model:
            return
           
        options = QFileDialog.Options()
        file_path, _ = QFileDialog.getSaveFileName(
            self, "–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –º–æ–¥–µ–ª—å", "", "Model Files (*.pt)", options=options
        )
       
        if not file_path:
            return
           
        try:
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
            state = {
                "model_state": self.model.state_dict(),
                "config": {
                    "dim": self.model.dim,
                    "vocab_size": self.model.head.out_features,
                    "num_layers": len(self.model.layers)
                },
                "tokenizer": self.processor.tokenizer.get_vocab()
            }
           
            torch.save(state, file_path)
            self.statusBar().showMessage(f"–ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {os.path.basename(file_path)}")
        except Exception as e:
            self.statusBar().showMessage(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è: {str(e)}")
   
    def load_model(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —Å –¥–∏—Å–∫–∞"""
        options = QFileDialog.Options()
        file_path, _ = QFileDialog.getOpenFileName(
            self, "–ó–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å", "", "Model Files (*.pt)", options=options
        )
       
        if not file_path:
            return
           
        try:
            # –ó–∞–≥—Ä—É–∑–∫–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è
            state = torch.load(file_path, map_location=torch.device('cpu'))
            config = state["config"]
           
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏
            self.model = NeuroSparseTransformer(
                vocab_size=config["vocab_size"],
                dim=config["dim"],
                num_layers=config["num_layers"]
            )
            self.model.load_state_dict(state["model_state"])
           
            # –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
            self.processor.tokenizer = AutoTokenizer.from_pretrained("gpt2")
            self.processor.tokenizer.add_tokens(list(state["tokenizer"].keys()))
           
            self.statusBar().showMessage(f"–ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {os.path.basename(file_path)}")
        except Exception as e:
            self.statusBar().showMessage(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {str(e)}")
   
    def consolidate_model(self):
        """–ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –∫–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏—è –º–æ–¥–µ–ª–∏"""
        if self.model:
            neuro_consolidation(self.model)
            self.statusBar().showMessage("–ö–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏—è –∑–Ω–∞–Ω–∏–π –≤—ã–ø–æ–ª–Ω–µ–Ω–∞!")
           
    def closeEvent(self, event):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–∫—Ä—ã—Ç–∏—è –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è"""
        if self.training_thread and self.training_thread.isRunning():
            self.training_thread.stop()
            self.training_thread.wait()
        event.accept()

if __name__ == "__main__":
    app = QApplication([])
    app.setStyle("Fusion")  # –°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π —Å—Ç–∏–ª—å –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞
    window = NeuroSparseApp()
    window.show()
    app.exec() 