# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rDFCXA7qatgoAo5Hutl9qOBmnj8mYQew
"""

import os
import numpy as np
import pandas as pd
from glob import glob
from scipy.ndimage import gaussian_filter1d
from torch.utils.data import Dataset, DataLoader, random_split
import torch
import re
import cv2
import math
from tqdm import tqdm
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, roc_curve, auc, RocCurveDisplay
import matplotlib.pyplot as plt
import seaborn as sns
from termcolor import colored
import time
from collections import Counter
import random
from scipy.signal import resample

# Set random seeds for reproducibility
SEED = 42
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
torch.cuda.manual_seed(SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False


LANDMARK_INDICES = {
    "L_SHOULDER": 11, "R_SHOULDER": 12,
    "L_ELBOW": 13,    "R_ELBOW": 14,
    "L_WRIST": 15,    "R_WRIST": 16,
    "L_HIP": 23,      "R_HIP": 24,
    "L_KNEE": 25,     "R_KNEE": 26,
    "L_ANKLE": 27,    "R_ANKLE": 28,
}
LI = LANDMARK_INDICES

# Define exercise-specific angles
# Ensure class names ("Squat", "LegRaise", etc.) EXACTLY match your folder names for classes.
EXERCISE_ANGLE_DEFINITIONS = {
    "Squat": [
        ("L_Hip_Knee_Ankle", (LI["L_HIP"], LI["L_KNEE"], LI["L_ANKLE"])),
        ("R_Hip_Knee_Ankle", (LI["R_HIP"], LI["R_KNEE"], LI["R_ANKLE"])),
        ("L_Shoulder_Hip_Knee", (LI["L_SHOULDER"], LI["L_HIP"], LI["L_KNEE"])),
        ("R_Shoulder_Hip_Knee", (LI["R_SHOULDER"], LI["R_HIP"], LI["R_KNEE"])),
    ],
    "Leg raise": [
        ("L_Shoulder_Hip_Ankle", (LI["L_SHOULDER"], LI["L_HIP"], LI["L_ANKLE"])),
        ("R_Shoulder_Hip_Ankle", (LI["R_SHOULDER"], LI["R_HIP"], LI["R_ANKLE"])),
    ],
    "Arnold Press": [
        ("L_Shoulder_Elbow_Wrist", (LI["L_SHOULDER"], LI["L_ELBOW"], LI["L_WRIST"])),
        ("R_Shoulder_Elbow_Wrist", (LI["R_SHOULDER"], LI["R_ELBOW"], LI["R_WRIST"])),
        ("L_Elbow_Shoulder_Hip", (LI["L_ELBOW"], LI["L_SHOULDER"], LI["L_HIP"])),
        ("R_Elbow_Shoulder_Hip", (LI["R_ELBOW"], LI["R_SHOULDER"], LI["R_HIP"])),
    ],
    "Bench Dip": [
        ("L_Shoulder_Elbow_Wrist", (LI["L_SHOULDER"], LI["L_ELBOW"], LI["L_WRIST"])),
        ("R_Shoulder_Elbow_Wrist", (LI["R_SHOULDER"], LI["R_ELBOW"], LI["R_WRIST"])),
        ("L_Elbow_Shoulder_Hip", (LI["L_ELBOW"], LI["L_SHOULDER"], LI["L_HIP"])),
        ("R_Elbow_Shoulder_Hip", (LI["R_ELBOW"], LI["R_SHOULDER"], LI["R_HIP"])),
    ]
}

# Define the primary angle(s) and its movement characteristic for each exercise
# 'primary_angles_to_consider': A list of angle names that will be averaged for repetition detection.
# 'peak_state': 'high' if the repetition peak is at a large angle (e.g., Arnold Press),
#               'low' if the peak is at a small angle (e.g., Squat).
EXERCISE_PRIMARY_ANGLE = {
    "Squat": {"primary_angles_to_consider": ["L_Hip_Knee_Ankle", "R_Hip_Knee_Ankle"], "peak_state": "low"},
    "Leg raise": {"primary_angles_to_consider": ["L_Shoulder_Hip_Ankle", "R_Shoulder_Hip_Ankle"], "peak_state": "low"},
    "Arnold Press": {"primary_angles_to_consider": ["L_Shoulder_Elbow_Wrist", "R_Shoulder_Elbow_Wrist"], "peak_state": "high"},
    "Bench Dip": {"primary_angles_to_consider": ["L_Shoulder_Elbow_Wrist", "R_Shoulder_Elbow_Wrist"], "peak_state": "low"},
}

MAX_N_ANGLES_STATIC = 0
for exercise_name in EXERCISE_ANGLE_DEFINITIONS:
    if len(EXERCISE_ANGLE_DEFINITIONS[exercise_name]) > MAX_N_ANGLES_STATIC:
        MAX_N_ANGLES_STATIC = len(EXERCISE_ANGLE_DEFINITIONS[exercise_name])

def calculate_angle(p1, p2, p3):
    """Calculates the angle (in degrees) between three 2D points p1-p2-p3 (p2 is the vertex)."""
    p1 = np.array(p1)
    p2 = np.array(p2)
    p3 = np.array(p3)

    if np.isnan(p1).any() or np.isnan(p2).any() or np.isnan(p3).any():
        return np.nan
    v1 = p1 - p2
    v2 = p3 - p2
    dot_product = np.dot(v1, v2)
    mag_v1 = np.linalg.norm(v1)
    mag_v2 = np.linalg.norm(v2)
    if mag_v1 * mag_v2 == 0: return np.nan
    cosine_angle = np.clip(dot_product / (mag_v1 * mag_v2), -1.0, 1.0)
    angle_rad = np.arccos(cosine_angle)
    angle_deg = np.degrees(angle_rad)
    return angle_deg

class BehaviorKeypointDataset(Dataset):
    """
    Dataset class to load keypoint data from CSV files.
    This version collects video paths and processes them on demand in __getitem__,
    returning a list of all repetitions found in that video.
    """
    def __init__(self, root_dir, target_rep_len=50, visibility_thresh=0.5,
                 smooth_sigma=1.0, target_size=(224, 224)):
        self.target_rep_len = target_rep_len
        self.visibility_thresh = visibility_thresh
        self.smooth_sigma = smooth_sigma
        self.target_size = target_size
        self.all_video_paths = [] # Stores (csv_file_path, class_idx) for each video

        self.class_names = sorted(os.listdir(root_dir))
        self.class_to_idx = {name: idx for idx, name in enumerate(self.class_names)}
        self.idx_to_class = {idx: name for name, idx in self.class_to_idx.items()}
        self.num_joints = 33

        self.MAX_N_ANGLES = 0
        for cn in self.class_names:
            if cn in EXERCISE_ANGLE_DEFINITIONS:
                num_angles_for_class = len(EXERCISE_ANGLE_DEFINITIONS[cn])
                if num_angles_for_class > self.MAX_N_ANGLES:
                    self.MAX_N_ANGLES = num_angles_for_class
            else:
                print(f"Warning: Class name '{cn}' from dataset not found in EXERCISE_ANGLE_DEFINITIONS.")
        if self.MAX_N_ANGLES == 0:
            print("Warning: MAX_N_ANGLES is 0. No angles will be calculated.")
            self.MAX_N_ANGLES = MAX_N_ANGLES_STATIC

        self._collect_all_video_paths(root_dir)

    def _collect_all_video_paths(self, root_dir):
        """Collects all CSV file paths and their corresponding class indices."""
        for class_idx, class_name in enumerate(self.class_names):
            class_path = os.path.join(root_dir, class_name)
            csv_files = glob(os.path.join(class_path, "*.csv"))
            for csv_file in csv_files:
                self.all_video_paths.append((csv_file, class_idx))

    def _calculate_angles_for_frame(self, keypoints_frame, class_name):
        """Calculates angles for a single frame based on exercise definition."""
        angle_definitions_for_class = EXERCISE_ANGLE_DEFINITIONS.get(class_name)
        if not angle_definitions_for_class:
            return []

        angles = []
        for angle_name, (idx1, idx2, idx3) in angle_definitions_for_class:
            if all(idx < len(keypoints_frame) for idx in [idx1, idx2, idx3]):
                p1, p2, p3 = keypoints_frame[idx1], keypoints_frame[idx2], keypoints_frame[idx3]
                angles.append(calculate_angle(p1, p2, p3))
            else:
                angles.append(np.nan)
        return np.array(angles)

    def _calculate_all_angles(self, keypoints_data, class_name):
        """Calculates all defined angles for all frames in a video."""
        T = keypoints_data.shape[0]
        angles_all_frames = np.full((T, self.MAX_N_ANGLES), np.nan, dtype=np.float32)

        if class_name not in EXERCISE_ANGLE_DEFINITIONS:
            return angles_all_frames

        for t in range(T):
            frame_kps = keypoints_data[t]
            calculated_angles = self._calculate_angles_for_frame(frame_kps, class_name)
            if len(calculated_angles) > 0:
                angles_all_frames[t, :len(calculated_angles)] = calculated_angles
        return angles_all_frames

    def _load_keypoints_coords(self, csv_file):
        """Loads keypoint coordinates from a CSV file."""
        df = pd.read_csv(csv_file)
        keypoints_all_frames = []
        for _, row in df.iterrows():
            coords_frame = []
            row_values = row.values

            if len(row_values) < 1 + 1 * 4: # Ensure at least one keypoint's data is present
                continue

            for j in range(33): # Iterate through all 33 joints
                start_idx = 1 + j * 4 # Column 0 is frame_id, then x,y,z,v for each joint
                end_idx = start_idx + 4

                if end_idx > len(row_values):
                    # If not enough columns for remaining joints, fill with NaN
                    coords_frame.extend([[np.nan, np.nan]] * (33 - j))
                    break

                try:
                    x, y, z, v = row_values[start_idx:end_idx]
                except IndexError:
                    # Should not happen if end_idx check is correct, but as a safeguard
                    coords_frame.extend([[np.nan, np.nan]] * (33 - j))
                    break

                try:
                    v_float = float(v)
                except ValueError:
                    v_float = np.nan # Handle non-numeric visibility

                # Check for NaN visibility or visibility below threshold
                if pd.isna(v_float) or v_float < self.visibility_thresh:
                    coords_frame.append([np.nan, np.nan])
                else:
                    try:
                        x_float, y_float = float(x), float(y)
                        coords_frame.append([x_float, y_float])
                    except ValueError:
                        coords_frame.append([np.nan, np.nan]) # Handle non-numeric coordinates

            if len(coords_frame) < 33:
                # Pad if fewer than 33 joints were processed (e.g., malformed row)
                coords_frame.extend([[np.nan, np.nan]] * (33 - len(coords_frame)))
            keypoints_all_frames.append(coords_frame[:33]) # Ensure only 33 joints are taken

        keypoints_np = np.array(keypoints_all_frames, dtype=np.float32)
        if keypoints_np.ndim == 1 and keypoints_np.shape[0] == 0:
            return np.empty((0, 33, 2), dtype=np.float32) # Return empty 3D array if no data
        return keypoints_np

    def _get_original_size_from_filename(self, filename):
        """Tries to infer original video dimensions from the corresponding video file."""
        video_path_base = os.path.splitext(filename)[0]
        common_exts = ['.mp4', '.avi', '.mov', '.mkv', '.MP4', '.AVI', '.MOV', '.MKV']
        found_video = False
        video_path = ""
        for ext in common_exts:
            potential_video_path = video_path_base + ext
            if os.path.exists(potential_video_path):
                video_path = potential_video_path
                found_video = True
                break
        if not found_video:
            # Fallback to common video resolution if video file not found
            return 1920, 1080
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            return 1920, 1080
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        cap.release()
        if width == 0 or height == 0:
            return 1920, 1080 # Fallback if dimensions are zero
        return width, height

    def _normalize_keypoints(self, keypoints, csv_file):
        """Normalizes keypoint coordinates to target_size dimensions."""
        orig_w, orig_h = self._get_original_size_from_filename(csv_file)
        if orig_w is None or orig_h is None or orig_w == 0 or orig_h == 0:
            # If original size is invalid, return keypoints as is (cannot normalize)
            return keypoints
        target_w, target_h = self.target_size
        scale_x = target_w / orig_w
        scale_y = target_h / orig_h
        keypoints_normalized = keypoints.copy()
        if not np.isnan(scale_x) and not np.isinf(scale_x) and \
           not np.isnan(scale_y) and not np.isinf(scale_y):
            keypoints_normalized[:, :, 0] *= scale_x
            keypoints_normalized[:, :, 1] *= scale_y
        return keypoints_normalized

    def _smooth_data(self, data_array):
        """Applies Gaussian smoothing to keypoint or angle data, handling NaNs."""
        if data_array.shape[0] == 0:
            return data_array

        if data_array.ndim == 3: # Keypoints (T, J, C)
            num_features = data_array.shape[1]
            num_dims = data_array.shape[2]
        elif data_array.ndim == 2: # Angles (T, A)
            num_features = data_array.shape[1]
            num_dims = 1
        else:
            raise ValueError("Data for smoothing must be 2D (angles) or 3D (keypoints).")

        smoothed_data = data_array.copy()
        for j in range(num_features):
            if data_array.ndim == 2: # Angles
                data_series = smoothed_data[:, j]
                nan_mask = np.isnan(data_series)
                if nan_mask.all():
                    smoothed_data[:, j] = 0.0 # If all NaNs, fill with 0
                    continue
                if nan_mask.any():
                    # Interpolate NaNs using valid surrounding data
                    valid_indices = np.flatnonzero(~nan_mask)
                    nan_indices = np.flatnonzero(nan_mask)
                    if len(valid_indices) > 0:
                        data_series[nan_indices] = np.interp(nan_indices, valid_indices, data_series[valid_indices])
                    else:
                        data_series.fill(0.0) # Fallback if no valid points for interp
                smoothed_data[:, j] = gaussian_filter1d(data_series, sigma=self.smooth_sigma)
            else: # Keypoints
                for d in range(num_dims):
                    data_series = smoothed_data[:, j, d]
                    nan_mask = np.isnan(data_series)
                    if nan_mask.all():
                        smoothed_data[:, j, d] = 0.0
                        continue
                    if nan_mask.any():
                        valid_indices = np.flatnonzero(~nan_mask)
                        nan_indices = np.flatnonzero(nan_mask)
                        if len(valid_indices) > 0:
                            data_series[nan_indices] = np.interp(nan_indices, valid_indices, data_series[valid_indices])
                        else:
                            data_series.fill(0.0)
                    smoothed_data[:, j, d] = gaussian_filter1d(data_series, sigma=self.smooth_sigma)
        return smoothed_data

    def _segment_by_repetition(self, angles_data, class_name):
        """
        Segments the data into repetitions using a state machine based on primary angles.
        Returns a list of (start_frame, end_frame) tuples for each repetition.
        """
        rep_indices = []

        if class_name not in EXERCISE_PRIMARY_ANGLE:
            return []

        primary_angle_info = EXERCISE_PRIMARY_ANGLE[class_name]
        primary_angle_names = primary_angle_info["primary_angles_to_consider"]
        peak_state_type = primary_angle_info["peak_state"]

        angle_definitions = EXERCISE_ANGLE_DEFINITIONS.get(class_name, [])

        # Map angle names to their indices in the angles_data array
        angle_name_to_idx = {name: i for i, (name, _) in enumerate(angle_definitions)}

        # Collect the actual angle series for the primary angles
        relevant_angle_series = []
        for p_angle_name in primary_angle_names:
            if p_angle_name in angle_name_to_idx:
                idx = angle_name_to_idx[p_angle_name]
                if idx < angles_data.shape[1]: # Ensure index is within bounds
                    relevant_angle_series.append(angles_data[:, idx])

        if not relevant_angle_series:
            return []

        # Combine the relevant angle series by taking the mean (handling NaNs)
        combined_angle_series = np.nanmean(np.array(relevant_angle_series), axis=0)

        nan_mask = np.isnan(combined_angle_series)
        if nan_mask.any():
            if nan_mask.all(): return []
            valid_indices = np.flatnonzero(~nan_mask)
            nan_indices = np.flatnonzero(nan_mask)
            # Interpolate NaN values before smoothing/thresholding
            if len(valid_indices) > 0:
                combined_angle_series[nan_indices] = np.interp(nan_indices, valid_indices, combined_angle_series[valid_indices])
            else:
                combined_angle_series.fill(0.0) # If all are NaN, fill with 0

        min_angle, max_angle = np.min(combined_angle_series), np.max(combined_angle_series)
        angle_range = max_angle - min_angle

        # If the range of motion is too small, it's likely not a full repetition.
        if angle_range < 5: # Threshold for minimum movement
            return []

        # Define thresholds for state transitions (25% and 75% of the range)
        thresh_low = min_angle + angle_range * 0.25
        thresh_high = min_angle + angle_range * 0.75

        start_frame = 0
        # Initialize state based on the peak_state_type.
        # If 'low' peak, we start in 'high' state, waiting for angle to drop.
        # If 'high' peak, we start in 'low' state, waiting for angle to rise.
        if peak_state_type == 'low':
            state = 'high'
        else: # 'high' peak_state_type
            state = 'low'

        # State machine for repetition detection
        for i in range(1, len(combined_angle_series)):
            angle = combined_angle_series[i]

            if peak_state_type == 'low': # e.g., Squat (low angle at peak of repetition)
                if state == 'high' and angle < thresh_low:
                    state = 'low' # Entered the 'low' phase of the repetition
                elif state == 'low' and angle > thresh_high:
                    state = 'high' # Completed a repetition, returned to 'high' start position
                    end_frame = i
                    rep_indices.append((start_frame, end_frame))
                    start_frame = i # Start new repetition from current frame
            else: # 'high' peak_state_type, e.g., Arnold Press (high angle at peak of repetition)
                if state == 'low' and angle > thresh_high:
                    state = 'high' # Entered the 'high' phase of the repetition
                elif state == 'high' and angle < thresh_low:
                    state = 'low' # Completed a repetition, returned to 'low' start position
                    end_frame = i
                    rep_indices.append((start_frame, end_frame))
                    start_frame = i # Start new repetition from current frame

        return rep_indices

    def __len__(self):
        """Returns the number of video files."""
        return len(self.all_video_paths)

    def __getitem__(self, idx):
        """
        Loads and processes a single video, returning a list of all detected repetitions
        from that video.
        """
        csv_file, class_idx = self.all_video_paths[idx]
        class_name = self.idx_to_class[class_idx]

        try:
            keypoints_coords = self._load_keypoints_coords(csv_file)

            if keypoints_coords.shape[0] < 10: # Minimum frames to consider for a repetition
                return [] # Return empty list if video is too short

            keypoints_normalized = self._normalize_keypoints(keypoints_coords.copy(), csv_file)
            angles_calculated = self._calculate_all_angles(keypoints_normalized, class_name)

            keypoints_smoothed = self._smooth_data(keypoints_normalized)
            angles_smoothed = self._smooth_data(angles_calculated)

            # If all keypoints or angles are NaN after smoothing, skip
            if np.isnan(keypoints_smoothed).all() or np.isnan(angles_smoothed).all():
                return []

            repetition_indices = self._segment_by_repetition(angles_smoothed, class_name)

            video_repetitions = []
            for start_frame, end_frame in repetition_indices:
                rep_kpts = keypoints_smoothed[start_frame:end_frame]
                rep_angles = angles_smoothed[start_frame:end_frame]

                # Resample repetitions to a fixed length
                if rep_kpts.shape[0] > 1: # Ensure there's enough data to resample
                    resampled_kpts = resample(rep_kpts, self.target_rep_len, axis=0)
                    resampled_angles = resample(rep_angles, self.target_rep_len, axis=0)

                    video_repetitions.append((
                        torch.tensor(resampled_kpts, dtype=torch.float32),
                        torch.tensor(resampled_angles, dtype=torch.float32),
                        class_idx
                    ))
            return video_repetitions # Return a list of all repetitions from this video
        except Exception as e:
            print(f"Error processing {csv_file} for class {class_name}: {e}")
            return [] # Return empty list on error


# --- START Data Augmentation Methods ---
# These methods remain the same as they operate on individual samples
# and are applied in the SubDataset wrapper.
def _apply_rotation(keypoints, max_angle=5):
    """
    Applies a random 2D rotation to keypoints.
    keypoints: (T, J, 2) tensor of keypoints (x, y)
    max_angle: maximum rotation angle in degrees (e.g., 5 for -5 to +5 degrees)
    """
    if keypoints.shape[0] == 0 or torch.isnan(keypoints).all():
        return keypoints

    angle = (torch.rand(1).item() - 0.5) * 2 * max_angle # Random angle between -max_angle and +max_angle
    angle_rad = torch.deg2rad(torch.tensor(angle))

    # Calculate centroid for rotation, ignoring NaN values
    non_nan_mask = ~torch.isnan(keypoints).any(dim=-1) # (T, J) mask
    if not non_nan_mask.any(): # If all frames/joints are NaN, return as is
        return keypoints

    # Calculate centroid only from non-NaN valid keypoints
    valid_kpts_coords = keypoints[non_nan_mask] # (N_valid_points, 2)
    if valid_kpts_coords.numel() == 0:
        return keypoints # No valid points to calculate centroid

    centroid = valid_kpts_coords.mean(dim=0, keepdim=True) # (1, 2)

    # Move keypoints to origin, rotate, then move back
    kpts_centered = keypoints - centroid

    rotation_matrix = torch.tensor([
        [torch.cos(angle_rad), -torch.sin(angle_rad)],
        [torch.sin(angle_rad),  torch.cos(angle_rad)]
    ], dtype=torch.float32, device=keypoints.device) # Ensure matrix is on same device as keypoints

    # Apply rotation: (T, J, 2) @ (2, 2)
    rotated_kpts = torch.matmul(kpts_centered, rotation_matrix) + centroid

    # Keep original NaN positions
    rotated_kpts[torch.isnan(keypoints)] = torch.nan
    return rotated_kpts


def _apply_scaling(keypoints, scale_range=(0.95, 1.05)):
    """
     Applies a random 2D scaling to keypoint coordinates.
    keypoints: (T, J, 2) tensor of keypoints (x, y)
    scale_range: (min_scale, max_scale) tuple
    """
    if keypoints.shape[0] == 0 or torch.isnan(keypoints).all():
        return keypoints

    scale_factor = torch.rand(1).item() * (scale_range[1] - scale_range[0]) + scale_range[0]

    # Scale relative to the centroid, ignoring NaN values
    non_nan_mask = ~torch.isnan(keypoints).any(dim=-1)
    if not non_nan_mask.any():
        return keypoints

    valid_kpts_coords = keypoints[non_nan_mask]
    if valid_kpts_coords.numel() == 0:
        return keypoints

    centroid = valid_kpts_coords.mean(dim=0, keepdim=True)

    scaled_kpts = (keypoints - centroid) * scale_factor + centroid

    # Keep original NaN positions
    scaled_kpts[torch.isnan(keypoints)] = torch.nan
    return scaled_kpts

def _add_noise(keypoints, noise_std=0.001):
    """
    Adds Gaussian noise to keypoint coordinates.
    keypoints: (T, J, 2) tensor of keypoints (x, y)
    noise_std: standard deviation of the Gaussian noise
    """
    if keypoints.shape[0] == 0 or torch.isnan(keypoints).all():
        return keypoints

    noise = torch.randn_like(keypoints) * noise_std
    noisy_kpts = keypoints + noise

    # Keep original NaN positions: noise should only be added to valid points
    noisy_kpts[torch.isnan(keypoints)] = torch.nan
    return noisy_kpts

def _apply_horizontal_flip(keypoints, flip_probability=0.5, image_width=256):
    """
    Applies horizontal flip to keypoints with a given probability.
    keypoints: (T, J, 2) tensor of keypoints (x, y) - assumed normalized to [0, image_width-1]
    image_width: The width of the image space the keypoints are normalized to (e.g., target_size[0])
    """
    if torch.rand(1).item() < flip_probability:
        flipped_keypoints = keypoints.clone()
        # Corrected flip: assuming coordinates are in pixel space [0, image_width-1]
        flipped_keypoints[:, :, 0] = image_width - 1 - flipped_keypoints[:, :, 0]

        # Define left-to-right landmark mapping for swapping
        left_to_right_map = {
            LI["L_SHOULDER"]: LI["R_SHOULDER"],
            LI["L_ELBOW"]: LI["R_ELBOW"],
            LI["L_WRIST"]: LI["R_WRIST"],
            LI["L_HIP"]: LI["R_HIP"],
            LI["L_KNEE"]: LI["R_KNEE"],
            LI["L_ANKLE"]: LI["R_ANKLE"],
            # Add more pairs as needed based on your LANDMARK_INDICES
        }
        # Create a reverse map for convenience
        right_to_left_map = {v: k for k, v in left_to_right_map.items()}

        # Combine maps for comprehensive swapping
        all_swaps = {**left_to_right_map, **right_to_left_map}

        # Perform the swap
        temp_kpts = flipped_keypoints.clone() # Use a temporary copy to avoid overwriting during swaps
        for l_idx, r_idx in left_to_right_map.items():
            if l_idx < 33 and r_idx < 33: # Ensure indices are valid
                flipped_keypoints[:, l_idx, :] = temp_kpts[:, r_idx, :]
                flipped_keypoints[:, r_idx, :] = temp_kpts[:, l_idx, :]
        return flipped_keypoints
    return keypoints
# --- END Data Augmentation Methods ---


# Custom class for sub-datasets to apply transformations conditionally
class VideoRepetitionDataset(Dataset):
    """
    A dataset that holds pre-processed repetitions and applies transformations if specified.
    """
    def __init__(self, samples, transform=False, target_size=(256, 256)):
        self.samples = samples
        self.transform = transform
        self.target_size = target_size

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        keypoints, angles, label = self.samples[idx]

        if self.transform:
            # Apply data augmentation only to the training set
            keypoints = _apply_rotation(keypoints, max_angle=5)
            keypoints = _apply_scaling(keypoints, scale_range=(0.95, 1.05))
            keypoints = _add_noise(keypoints, noise_std=0.001)
            # Pass image_width for correct horizontal flipping
            keypoints = _apply_horizontal_flip(keypoints, flip_probability=0.5, image_width=self.target_size[0])

        return keypoints, angles, label

class DepthwiseConv(nn.Module):
    def __init__(self, channels, kernel_size=3):
        super().__init__()
        self.conv = nn.Conv1d(channels, channels, kernel_size, padding=kernel_size//2, groups=channels)

    def forward(self, x):
        return self.conv(x)

class PointwiseConv(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()
        self.conv = nn.Conv1d(in_channels, out_channels, kernel_size=1, stride=1)

    def forward(self, x):
        return self.conv(x)

class SpatialAttention(nn.Module):
    def __init__(self, in_channels):
        super().__init__()
        self.query = nn.Linear(in_channels, in_channels // 2 if in_channels >1 else 1)
        self.key = nn.Linear(in_channels, in_channels // 2 if in_channels > 1 else 1)
        self.value = nn.Linear(in_channels, in_channels)
        self.gamma = nn.Parameter(torch.zeros(1))

    def forward(self, x):
        B, C, J = x.size()

        q_in = x.permute(0, 2, 1)
        k_in = x.permute(0, 2, 1)
        v_in = x.permute(0, 2, 1)

        Q = self.query(q_in)
        K = self.key(k_in)
        V = self.value(v_in)

        attn_scores = torch.bmm(Q, K.transpose(1, 2)) / ( (C//2 if C>1 else 1) ** 0.5)
        attn_weights = torch.softmax(attn_scores, dim=-1)

        out = torch.bmm(attn_weights, V)
        out = out.permute(0, 2, 1)

        return x + self.gamma * out

class LocalSparseAttention(nn.Module):
    def __init__(self, embed_dim, window_size=7):
        super().__init__()
        assert window_size % 2 == 1, "Window size must be odd"
        self.embed_dim = embed_dim
        self.window_size = window_size
        self.pad = window_size // 2

        self.query = nn.Linear(embed_dim, embed_dim)
        self.key = nn.Linear(embed_dim, embed_dim)
        self.value = nn.Linear(embed_dim, embed_dim)

    def forward(self, x):
        B, D, L = x.shape
        x_transposed = x.permute(0, 2, 1)

        Q_proj = self.query(x_transposed)
        K_proj = self.key(x_transposed)
        V_proj = self.value(x_transposed)

        K_padded = F.pad(K_proj.permute(0,2,1), (self.pad, self.pad), mode='constant', value=0)
        V_padded = F.pad(V_proj.permute(0,2,1), (self.pad, self.pad), mode='constant', value=0)

        K_windows = K_padded.unfold(dimension=2, size=self.window_size, step=1)
        V_windows = V_padded.unfold(dimension=2, size=self.window_size, step=1)

        Q_unsqueeze = Q_proj.unsqueeze(-2)
        K_windows_permute = K_windows.permute(0, 2, 3, 1)
        V_windows_permute = V_windows.permute(0, 2, 3, 1)

        attn_scores = torch.matmul(Q_unsqueeze, K_windows_permute.transpose(-2, -1)) / (D ** 0.5)
        attn_weights = F.softmax(attn_scores, dim=-1)

        out_windows = torch.matmul(attn_weights, V_windows_permute)
        out_squeezed = out_windows.squeeze(-2)

        return x + out_squeezed.permute(0,2,1)

class ConvAttentionBlock(nn.Module):
    def __init__(self, in_channels, out_channels, num_joints_as_seq_len=33):
        super().__init__()
        self.depthwise = DepthwiseConv(in_channels, kernel_size=3)
        self.norm1 = nn.BatchNorm1d(in_channels)
        self.relu = nn.ReLU()

        self.spatial_attn = SpatialAttention(in_channels)
        self.norm2 = nn.BatchNorm1d(in_channels)

        self.local_sparse_attn = LocalSparseAttention(in_channels, window_size=min(7, num_joints_as_seq_len if num_joints_as_seq_len % 2 != 0 else num_joints_as_seq_len -1))
        self.norm3 = nn.BatchNorm1d(in_channels)

        self.pointwise = PointwiseConv(in_channels, out_channels)
        self.norm4 = nn.BatchNorm1d(out_channels)

        self.shortcut = nn.Conv1d(in_channels, out_channels, kernel_size=1, stride=1) if in_channels != out_channels else None

    def forward(self, x):
        shortcut_x = x
        if self.shortcut:
            shortcut_x = self.shortcut(x)

        x = self.depthwise(x)
        x = self.norm1(x)
        x = self.relu(x)

        x_sa = self.spatial_attn(x)
        x = x + x_sa
        x = self.norm2(x)
        x = self.relu(x)

        x_lsa = self.local_sparse_attn(x)
        x = x + x_lsa
        x = self.norm3(x)
        x = self.relu(x)

        x = self.pointwise(x)
        x = self.norm4(x)

        x = x + shortcut_x
        return self.relu(x)

# -------- Temporal Attention -------- #

class TemporalSelfAttention(nn.Module):
    def __init__(self, embed_dim, num_heads=4):
        super().__init__()
        self.attn = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=num_heads, batch_first=True)
        self.norm = nn.LayerNorm(embed_dim)

    def forward(self, x):
        attn_output, _ = self.attn(x, x, x)
        return self.norm(x + attn_output)

# -------- Simplified Final Model -------- #
class CNNGRUClassifier(nn.Module):
    def __init__(self, num_joints=33, input_dim_coords=2, max_n_angles=4,
                 base_channels=16, num_conv_layers=1, # Simplified: fewer channels, fewer layers
                 gru_hidden_dim=128, num_classes=6, dropout_rate=0.4): # Reduced GRU hidden, 1 layer, slightly less dropout
        super().__init__()
        self.max_n_angles = max_n_angles
        self.num_joints = num_joints

        conv_layers_seq = []
        current_in_channels_coords = input_dim_coords
        for i in range(num_conv_layers):
            out_channels_coords = base_channels * (2 ** i)
            conv_layers_seq.append(ConvAttentionBlock(current_in_channels_coords, out_channels_coords, num_joints_as_seq_len=self.num_joints))
            current_in_channels_coords = out_channels_coords
        final_conv_output_channels = current_in_channels_coords

        self.conv_blocks = nn.Sequential(*conv_layers_seq)
        self.pool = nn.AdaptiveAvgPool1d(1)

        gru_input_dim = final_conv_output_channels + self.max_n_angles
        self.gru = nn.GRU(gru_input_dim, gru_hidden_dim, batch_first=True, num_layers=1, dropout=dropout_rate if 1 > 1 else 0) # Dropout only if num_layers > 1
        self.temporal_attn = TemporalSelfAttention(gru_hidden_dim, num_heads=4) # Using 4 heads for temporal attention

        # Simplified FC layers
        self.fc1 = nn.Linear(gru_hidden_dim, 64) # Reduced intermediate dimension
        self.norm_fc1 = nn.BatchNorm1d(64)
        self.dropout_fc1 = nn.Dropout(dropout_rate)

        self.classifier = nn.Linear(64, num_classes) # Direct to classifier

    def forward(self, x_kpts, x_angles):
        B, T, J, C_coords = x_kpts.shape
        assert J == self.num_joints, f"Input num_joints {J} does not match model num_joints {self.num_joints}"

        x_kpts_proc = x_kpts.reshape(B * T, J, C_coords).permute(0, 2, 1)

        x_kpts_proc = self.conv_blocks(x_kpts_proc)
        x_kpts_proc = self.pool(x_kpts_proc)
        x_kpts_features = x_kpts_proc.squeeze(-1)
        x_kpts_features = x_kpts_features.view(B, T, -1)

        x_angles_features = x_angles.view(B, T, self.max_n_angles)

        x_combined_features = torch.cat((x_kpts_features, x_angles_features), dim=2)

        x_gru_out, _ = self.gru(x_combined_features)
        x_attn_out = self.temporal_attn(x_gru_out)

        x_pooled = x_attn_out[:, -1, :] # Take the last hidden state after attention

        # Apply simplified FC layers with Batch Normalization and Dropout
        x_fc_out = self.dropout_fc1(F.relu(self.norm_fc1(self.fc1(x_pooled))))

        return self.classifier(x_fc_out)

dataset_root_dir = "/content/drive/MyDrive/FINAL DATASET/Keypoints"
if not os.path.exists(dataset_root_dir):
    print(f"ERROR: Dataset directory not found at {dataset_root_dir}")
    print("Please ensure your Google Drive is mounted and the path is correct.")
    full_video_dataset = None
else:
    # Initialize BehaviorKeypointDataset to collect video paths
    full_video_dataset = BehaviorKeypointDataset(
        root_dir=dataset_root_dir,
        target_rep_len=80, # Each repetition will be resampled to this length
        target_size=(256, 256)
    )

if full_video_dataset and len(full_video_dataset) > 0:
    # Get all video indices
    all_video_indices = list(range(len(full_video_dataset)))
    random.shuffle(all_video_indices) # Shuffle video indices for random split

    # Split ratios (e.g., 70% train, 15% val, 15% test)
    train_ratio = 0.7
    val_ratio = 0.15
    test_ratio = 0.15

    # Calculate split sizes based on videos
    total_videos = len(all_video_indices)
    train_video_size = int(train_ratio * total_videos)
    val_video_size = int(val_ratio * total_videos)
    test_video_size = total_videos - train_video_size - val_video_size

    # Adjust sizes to ensure they sum up to total_videos and are non-negative
    if train_video_size < 0: train_video_size = 0
    if val_video_size < 0: val_video_size = 0
    if test_video_size < 0: test_video_size = 0

    current_sum = train_video_size + val_video_size + test_video_size
    if current_sum != total_videos:
        train_video_size += (total_videos - current_sum)
        if train_video_size < 0:
            print("Warning: Adjusted train_video_size is negative. Check dataset size and ratios.")

    # Split video indices
    train_video_indices = all_video_indices[:train_video_size]
    val_video_indices = all_video_indices[train_video_size : train_video_size + val_video_size]
    test_video_indices = all_video_indices[train_video_size + val_video_size :]

    # Collect all repetitions for each split by processing videos
    train_samples = []
    for idx in tqdm(train_video_indices, desc="Preparing Train Data"):
        reps = full_video_dataset[idx] # This calls __getitem__ to process one video
        if reps: # Only extend if repetitions were found for the video
            train_samples.extend(reps)

    val_samples = []
    for idx in tqdm(val_video_indices, desc="Preparing Val Data"):
        reps = full_video_dataset[idx]
        if reps:
            val_samples.extend(reps)

    test_samples = []
    for idx in tqdm(test_video_indices, desc="Preparing Test Data"):
        reps = full_video_dataset[idx]
        if reps:
            test_samples.extend(reps)

    # Create actual datasets with transformation flags using the new wrapper
    train_dataset = VideoRepetitionDataset(train_samples, transform=True, target_size=full_video_dataset.target_size)
    val_dataset = VideoRepetitionDataset(val_samples, transform=False, target_size=full_video_dataset.target_size)
    test_dataset = VideoRepetitionDataset(test_samples, transform=False, target_size=full_video_dataset.target_size)

    print(f"Train repetitions: {len(train_dataset)}, Val repetitions: {len(val_dataset)}, Test repetitions: {len(test_dataset)}")

    # Create DataLoaders
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    BATCH_SIZE = 128

    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)
    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)

    # --- Calculate Class Weights (using only training labels for robustness) ---
    all_labels_for_weights = [sample[2] for sample in train_samples]
    label_counts = Counter(all_labels_for_weights)
    print("Class distribution in training set:", label_counts)
    print("Class names:", full_video_dataset.idx_to_class)

    num_classes = len(full_video_dataset.class_names)
    total_samples_for_weights = len(all_labels_for_weights)
    class_weights = torch.zeros(num_classes)
    for class_idx, count in label_counts.items():
        if count > 0: # Avoid division by zero if a class is missing in training set
            class_weights[class_idx] = total_samples_for_weights / (num_classes * count)
        else:
            class_weights[class_idx] = 0.0 # Assign 0 weight if class not present

    # Normalize weights (optional, but good practice)
    if class_weights.sum() > 0:
        class_weights = class_weights / class_weights.sum() * num_classes
    else:
        print("Warning: Sum of class weights is zero. All classes might be missing or counts are zero.")

    print("Calculated class weights:", class_weights)
    class_weights = class_weights.to(device)

else:
    print("Skipping data loading and weight calculation as dataset was not found or is empty.")
    train_loader, val_loader, test_loader = None, None, None
    class_weights = None

# --- Model Training ---
if train_loader and val_loader: # Check if loaders are defined
    LEARNING_RATE = 0.001
    DROPOUT_RATE = 0.5 # Adjusted dropout rate for simplified model

    model = CNNGRUClassifier(
        num_joints=33,
        input_dim_coords=2,
        max_n_angles=full_video_dataset.MAX_N_ANGLES,
        base_channels=32, # Reduced base channels
        num_conv_layers=1, # Reduced number of conv layers
        gru_hidden_dim=256, # Reduced GRU hidden dim
        num_classes=len(full_video_dataset.class_names),
        dropout_rate=DROPOUT_RATE
    ).to(device)

    # Use class_weights in the criterion
    criterion = nn.CrossEntropyLoss(weight=class_weights)
    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5)

    # Import the learning rate scheduler
    from torch.optim.lr_scheduler import ReduceLROnPlateau
    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=7)

    num_epochs = 100
    early_stopping_patience = 15
    best_val_loss = float('inf')
    epochs_no_improve = 0
    best_model_state = None

    train_losses, val_losses_history = [], []
    train_accs, val_accs_history = [], []

    # Color definitions
    COLOR_TRAIN = 'blue'
    COLOR_VAL = 'green'
    COLOR_EPOCH = 'yellow'
    COLOR_METRIC = 'cyan'

    print(colored("\n Starting Training...", 'magenta', attrs=['bold']))
    print(f"Epochs: {num_epochs} | Model: {model.__class__.__name__} | Device: {device}")
    print(f"Learning Rate: {LEARNING_RATE} | Batch Size: {BATCH_SIZE} | Dropout Rate: {DROPOUT_RATE}")
    print(f"Early Stopping Patience: {early_stopping_patience} epochs.\n")

    for epoch in range(num_epochs):
        # Training phase
        model.train()
        running_loss = 0.0
        correct_train, total_train = 0, 0

        train_iter = tqdm(train_loader,
                          desc=colored(f'Epoch {epoch+1}/{num_epochs} [T]', COLOR_EPOCH),
                          leave=False)

        for keypoints, angles, labels in train_iter:
            keypoints, angles, labels = keypoints.to(device), angles.to(device), labels.to(device)

            optimizer.zero_grad()
            outputs = model(keypoints, angles)
            loss = criterion(outputs, labels)
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()

            running_loss += loss.item()
            _, predicted_train = torch.max(outputs.data, 1)
            total_train += labels.size(0)
            correct_train += (predicted_train == labels).sum().item()

            train_iter.set_postfix({
                'loss': f"{running_loss/(train_iter.n+1):.4f}",
                'acc': f"{100*correct_train/total_train:.2f}%"}
            )

        train_loss_epoch = running_loss / len(train_loader)
        train_acc_epoch = 100 * correct_train / total_train
        train_losses.append(train_loss_epoch)
        train_accs.append(train_acc_epoch)

        # Validation phase
        model.eval()
        running_val_loss = 0.0
        correct_val, total_val = 0, 0

        val_iter = tqdm(val_loader,
                        desc=colored(f'Epoch {epoch+1}/{num_epochs} [V]', COLOR_VAL),
                        leave=False)

        with torch.no_grad():
            for keypoints, angles, labels in val_iter:
                keypoints, angles, labels = keypoints.to(device), angles.to(device), labels.to(device)
                outputs = model(keypoints, angles)
                loss = criterion(outputs, labels) # Use the weighted criterion here too for consistent loss reporting
                running_val_loss += loss.item()
                _, predicted_val = torch.max(outputs.data, 1)
                total_val += labels.size(0)
                correct_val += (predicted_val == labels).sum().item()
                val_iter.set_postfix({
                    'val_loss': f"{running_val_loss/(val_iter.n+1):.4f}",
                    'val_acc': f"{100*correct_val/total_val:.2f}%"}
                )

        val_loss_epoch = running_val_loss / len(val_loader)
        val_acc_epoch = 100 * correct_val / total_val
        val_losses_history.append(val_loss_epoch)
        val_accs_history.append(val_acc_epoch)

        scheduler.step(val_loss_epoch)

        print(colored(f"\n✨ Epoch {epoch+1}/{num_epochs} Summary:", COLOR_EPOCH, attrs=['bold']))
        print(colored("╭──────────────────────┬──────────────────────╮", 'white'))
        print(colored("│        Training      │      Validation      │", 'white'))
        print(colored("├──────────────────────┼──────────────────────┤", 'white'))
        print(colored(f"│ Loss: {train_loss_epoch:>10.4f}      │ Loss: {val_loss_epoch:>10.4f}      │", 'white'))
        print(colored(f"│ Acc:  {train_acc_epoch:>10.2f}%      │ Acc:  {val_acc_epoch:>10.2f}%      │", 'white'))
        print(colored("╰──────────────────────┴──────────────────────╯", 'white'))

        if val_loss_epoch < best_val_loss:
            best_val_loss = val_loss_epoch
            epochs_no_improve = 0
            best_model_state = model.state_dict()
            print(colored(f" New best validation loss: {best_val_loss:.4f}. Saving model...", 'green'))
        else:
            epochs_no_improve += 1
            print(colored(f" Validation loss did not improve for {epochs_no_improve} epoch(s). Best: {best_val_loss:.4f}", 'red'))

        if epochs_no_improve >= early_stopping_patience:
            print(colored(f"\n Early stopping triggered after {epoch+1} epochs. No improvement in validation loss for {early_stopping_patience} epochs.", 'red', attrs=['bold']))
            break

    print(colored(" Training Complete!", 'magenta', attrs=['bold']))
    if best_model_state:
        print(colored(" Loading best model state for evaluation.", 'yellow'))
        model.load_state_dict(best_model_state)
    else:
        print(colored(" No best model state saved (e.g., training stopped very early or did not improve). Using last model state.", 'yellow'))

    if model and full_video_dataset:
        model_save_path = "best_model.pt"
        torch.save(model.state_dict(), model_save_path)
        print(f"Model saved to {model_save_path}")

        import json
        class_names_path = "class_names.json"
        with open(class_names_path, 'w') as f:
            json.dump(full_video_dataset.class_names, f)
        print(f"Class names saved to {class_names_path}")
    else:
        print("Model or dataset not available, skipping model saving.")
    print(colored(f"Best Training Accuracy: {max(train_accs) if train_accs else 0:.2f}%", COLOR_TRAIN))
    print(colored(f"Best Validation Accuracy (at best val loss): {val_accs_history[val_losses_history.index(min(val_losses_history))] if val_losses_history else 0:.2f}% corresponding to Val Loss: {min(val_losses_history) if val_losses_history else float('inf'):.4f}", COLOR_VAL))
    print(colored(f"Overall Best Validation Accuracy: {max(val_accs_history) if val_accs_history else 0:.2f}%", COLOR_VAL))

else:
    print("Skipping model training as dataset was not loaded or is empty.")
    model = None

if model and test_loader: # Check if model and test_loader are defined
    def evaluate_model(model, dataloader, device, criterion_eval=None, phase="Test"):
        model.eval()
        all_preds = []
        all_probs = []
        all_labels = []
        total_time = 0.0
        total_samples = 0
        running_loss_eval = 0.0

        eval_iter = tqdm(dataloader,
                         desc=colored(f'Evaluating [{phase}]', 'cyan'),
                         leave=False)

        with torch.no_grad():
            for keypoints, angles, labels in eval_iter:
                keypoints = keypoints.to(device)
                angles = angles.to(device)
                labels = labels.to(device)

                start_time = time.time()
                outputs = model(keypoints, angles)
                batch_time = time.time() - start_time
                total_time += batch_time
                total_samples += labels.size(0)

                if criterion_eval:
                    loss = criterion_eval(outputs, labels)
                    running_loss_eval += loss.item()

                probs = torch.softmax(outputs, dim=1)
                _, preds = torch.max(outputs, 1)

                all_preds.extend(preds.cpu().numpy())
                all_probs.extend(probs.cpu().numpy())
                all_labels.extend(labels.cpu().numpy())

                eval_iter.set_postfix({
                    'avg_time_sample_ms': f"{(total_time/total_samples)*1000:.2f}"
                })

        avg_time_per_sample = total_time / total_samples if total_samples > 0 else 0
        eval_loss = running_loss_eval / len(dataloader) if criterion_eval and len(dataloader) > 0 else 0
        return np.array(all_preds), np.array(all_probs), np.array(all_labels), total_time, avg_time_per_sample, eval_loss

    def calculate_metrics_and_plot(y_true, y_pred, y_probs, total_time, avg_time_per_sample, eval_loss, class_names, num_classes, phase="Test"):
        print(f"\n{phase} Set Performance:")
        print(f" Total inference time: {total_time:.4f} seconds")
        print(f" Average time per sample: {avg_time_per_sample*1000:.4f} milliseconds")
        if eval_loss > 0:
            print(f" {phase} Loss: {eval_loss:.4f}")


        accuracy = np.mean(y_true == y_pred) * 100
        precision_macro = precision_score(y_true, y_pred, average='macro', zero_division=0)
        recall_macro = recall_score(y_true, y_pred, average='macro', zero_division=0)
        f1_macro = f1_score(y_true, y_pred, average='macro', zero_division=0)

        print(f" {phase} Accuracy: {accuracy:.2f}%")
        print(f" {phase} Precision : {precision_macro:.4f}")
        print(f" {phase} Recall : {recall_macro:.4f}")
        print(f" {phase} F1 Score : {f1_macro:.4f}")



        # ROC Curve (One-vs-Rest)
        fpr = dict()
        tpr = dict()
        roc_auc = dict()

        plt.figure(figsize=(10,8))
        for i in range(num_classes):
            fpr[i], tpr[i], _ = roc_curve((y_true == i).astype(int), y_probs[:, i])
            roc_auc[i] = auc(fpr[i], tpr[i])
            plt.plot(fpr[i], tpr[i], label=f'Class {class_names[i]} (AUC = {roc_auc[i]:.2f})')

        plt.plot([0, 1], [0, 1], 'k--')
        plt.xlim([0.0, 1.0])
        plt.ylim([0.0, 1.05])
        plt.xlabel('False Positive Rate')
        plt.ylabel('True Positive Rate')
        plt.title(f'Multi-class ROC Curve ({phase} Set)')
        plt.legend(loc="lower right")
        plt.show()

    # Use a simple CrossEntropyLoss for evaluation metrics, weights are for training bias.
    # If class_weights is None (e.g. dataset was empty), create a dummy one for criterion_eval
    if class_weights is None and full_video_dataset:
         num_classes_eval = len(full_video_dataset.class_names) if full_video_dataset else 2 # Default to 2 if no dataset
         criterion_eval = nn.CrossEntropyLoss().to(device)
    elif class_weights is not None:
         criterion_eval = nn.CrossEntropyLoss().to(device) # Unweighted for pure eval
    else: # full_dataset is also None
        criterion_eval = None



    test_preds, test_probs, test_labels, test_total_time, test_avg_time, test_set_loss = evaluate_model(
        model, test_loader, device, criterion_eval=criterion_eval, phase="Test"
    )
    calculate_metrics_and_plot(
        test_labels, test_preds, test_probs,
        test_total_time, test_avg_time, test_set_loss,
        full_video_dataset.class_names, len(full_video_dataset.class_names), phase="Test"
    )

def plot_confusion_matrix(model, loader):
    model.eval()
    all_preds = []
    all_labels = []

    with torch.no_grad():
        for keypoints, angles, labels in loader:
            keypoints = keypoints.to(device)
            angles = angles.to(device)          # Move angles to device
            labels = labels.to(device)
            outputs = model(keypoints, angles)
            _, preds = torch.max(outputs, 1)
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    cm = confusion_matrix(all_labels, all_preds)
    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues",
                xticklabels=full_video_dataset.class_names,
                yticklabels=full_video_dataset.class_names)
    plt.xlabel("Predicted")
    plt.ylabel("True")
    plt.title("Confusion Matrix")
    plt.show()

# Ensure model and necessary history lists exist before plotting
if model and test_loader and 'train_losses' in locals() and 'val_losses_history' in locals():
    plot_confusion_matrix(model, test_loader)

    plt.figure(figsize=(12, 4))
    plt.subplot(1, 2, 1)
    plt.plot(train_losses, label="Train Loss")
    # Corrected variable name
    plt.plot(val_losses_history, label="Validation Loss")
    plt.xlabel("Epoch")
    plt.ylabel("Loss")
    plt.legend()

    plt.subplot(1, 2, 2)
    plt.plot(train_accs, label="Train Accuracy")
    # Corrected variable name
    plt.plot(val_accs_history, label="Validation Accuracy")
    plt.xlabel("Epoch")
    plt.ylabel("Accuracy (%)")
    plt.legend()
    plt.show()
else:
    print("Skipping final confusion matrix and history plots as model was not trained or history data is missing.")