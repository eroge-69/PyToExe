import numpy as np
import pickle
import re
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split

# ØªÙ…ÛŒØ² Ú©Ø±Ø¯Ù† Ù…ØªÙ† ÙˆØ±ÙˆØ¯ÛŒ
def preprocess_text(text):
    text = text.lower()
    text = re.sub(r'[^\w\s\u0600-\u06FF]', '', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text

# ØªÙˆÙ„ÛŒØ¯ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ø­Ø³Ø§Ø³ÛŒ
happy_texts = [
    "Ø®ÛŒÙ„ÛŒ Ø´Ø§Ø¯Ù…", "Ø²Ù†Ø¯Ú¯ÛŒ Ù‚Ø´Ù†Ú¯Ù‡", "Ø­Ø³ Ø®ÙˆØ¨ÛŒ Ø¯Ø§Ø±Ù…", "Ø§Ù…Ø±ÙˆØ² ÙÙˆÙ‚â€ŒØ§Ù„Ø¹Ø§Ø¯Ù‡ Ø¨ÙˆØ¯", "Ù‡Ù…Ù‡ Ú†ÛŒ Ø¹Ø§Ù„ÛŒÙ‡",
    "Ù¾Ø± Ø§Ø² Ø§Ù†Ø±Ú˜ÛŒâ€ŒØ§Ù…", "Ø§Ø² Ù„Ø­Ø¸Ù‡ Ù„Ø­Ø¸Ù‡ Ø²Ù†Ø¯Ú¯ÛŒ Ù„Ø°Øª Ù…ÛŒâ€ŒØ¨Ø±Ù…", "Ø§Ù…ÛŒØ¯ ØªÙˆ Ø¯Ù„Ù…Ù‡", "Ø®ÙˆØ´Ø¨Ø®ØªÙ…", "Ø±ÙˆØ²Ù… Ø¨ÛŒâ€ŒÙ†Ø¸ÛŒØ± Ø¨ÙˆØ¯"
]

sad_texts = [
    "Ø­Ø§Ù„Ù… Ú¯Ø±ÙØªÙ‡â€ŒØ³Øª", "Ø§Ø­Ø³Ø§Ø³ Ù¾ÙˆÚ†ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ù…", "Ø¯Ù„Ú¯ÛŒØ±Ù…", "Ù‡Ù…Ù‡ Ú†ÛŒ Ø¨Ù‡Ù… Ø±ÛŒØ®ØªÙ‡", "Ø§Ù…Ø±ÙˆØ² Ø±ÙˆØ² Ø¨Ø¯ÛŒ Ø¨ÙˆØ¯",
    "Ø¨ÛŒâ€ŒØ§Ù†Ú¯ÛŒØ²Ù‡â€ŒØ§Ù…", "Ù†Ù…ÛŒâ€ŒØ¯ÙˆÙ†Ù… Ú†Ø±Ø§ Ù†Ø§Ø±Ø§Ø­ØªÙ…", "Ø­Ø³ ØªÙ†Ù‡Ø§ÛŒÛŒ Ø¯Ø§Ø±Ù…", "Ù‡Ù…Ù‡ Ú†ÛŒ Ø®Ø§Ú©Ø³ØªØ±ÛŒÙ‡", "Ø®Ø³ØªÙ…"
]

# ØªÙˆÙ„ÛŒØ¯ 600 Ø¯Ø§Ø¯Ù‡ (300 Ù…Ø«Ø¨ØªØŒ 300 Ù…Ù†ÙÛŒ)
happy_data = [preprocess_text(t + f" {i}") for i in range(30) for t in happy_texts][:300]
sad_data = [preprocess_text(t + f" {i}") for i in range(30) for t in sad_texts][:300]

all_texts = happy_data + sad_data
labels = [1]*300 + [0]*300

# ØªÙˆÚ©Ù†â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
word_tokenizer = Tokenizer(num_words=2500, oov_token="<UNK>")
word_tokenizer.fit_on_texts(all_texts)
text_sequences = word_tokenizer.texts_to_sequences(all_texts)
padded_texts = pad_sequences(text_sequences, padding='post', maxlen=20)

# Ø°Ø®ÛŒØ±Ù‡ ØªÙˆÚ©Ù†Ø§ÛŒØ²Ø±
with open("word_tokenizer.pkl", "wb") as f:
    pickle.dump(word_tokenizer, f, protocol=pickle.HIGHEST_PROTOCOL)

# ØªÙ‚Ø³ÛŒÙ… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ù‡ Ø¢Ù…ÙˆØ²Ø´ÛŒ Ùˆ Ø¢Ø²Ù…Ø§ÛŒØ´ÛŒ
X_train, X_test, y_train, y_test = train_test_split(padded_texts, labels, test_size=0.25, random_state=101)

# Ø³Ø§Ø®Øª Ù…Ø¯Ù„
emotion_model = Sequential([
    Embedding(input_dim=2500, output_dim=80, input_length=20),
    Dropout(0.4),
    LSTM(80, return_sequences=False),
    Dense(1, activation='sigmoid')
])

# Ú©Ø§Ù…Ù¾Ø§ÛŒÙ„ Ùˆ Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„
emotion_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
emotion_model.fit(X_train, np.array(y_train), epochs=25, batch_size=16, validation_split=0.15, verbose=2)

# Ø°Ø®ÛŒØ±Ù‡ Ù…Ø¯Ù„
emotion_model.save("emotion_analyzer.keras")

# ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§ÛŒ Ø§Ø­Ø³Ø§Ø³ÛŒ
def create_response(mood, confidence):
    if mood == "Ø®ÙˆØ´Ø­Ø§Ù„ ğŸ˜Š":
        responses = [
            "ÙˆØ§ÛŒ Ú†Ù‡ Ø­Ø³ Ø®ÙˆØ¨ÛŒ! ğŸ˜„ Ú†ÛŒ ØªÙˆØ±Ùˆ Ø§ÛŒÙ†Ù‚Ø¯Ø± Ø´Ø§Ø¯ Ú©Ø±Ø¯Ù‡ØŸ",
            "Ø¹Ø§Ù„ÛŒÙ‡! Ø¯ÙˆØ³Øª Ø¯Ø§Ø±ÛŒ Ø§Ø² Ø§ÛŒÙ† Ø­Ø³ Ù‚Ø´Ù†Ú¯ Ø¨ÛŒØ´ØªØ± Ø¨Ú¯ÛŒØŸ",
            "Ø§ÛŒÙ† Ø´Ø§Ø¯ÛŒ Ù…Ø³Ø±ÛŒÙ‡! ğŸ˜Š Ú†ÛŒØ²ÛŒ Ø¨Ø§Ø¹Ø«Ø´ Ø´Ø¯Ù‡ØŸ"
        ]
    else:
        responses = [
            "Ø§ÙˆÙˆÙ‡ØŒ Ø§Ù†Ú¯Ø§Ø± Ø­Ø§Ù„Øª Ø®ÙˆØ¨ Ù†ÛŒØ³Øª ğŸ˜” Ú†ÛŒØ²ÛŒ Ø´Ø¯Ù‡ØŸ Ø¨Ú¯Ùˆ Ø´Ø§ÛŒØ¯ Ú©Ù…Ú© Ú©Ù†Ù….",
            "Ù†Ø§Ø±Ø§Ø­Øª Ù†Ø¨Ø§Ø´ØŒ Ù…Ù† Ø§ÛŒÙ†Ø¬Ø§Ù… Ø¨Ø±Ø§Øª ğŸ¥º Ú†ÛŒ ØªÙˆ Ø¯Ù„ØªÙ‡ØŸ",
            "Ø§Ú¯Ù‡ Ø¨Ø®ÙˆØ§ÛŒ Ù…ÛŒâ€ŒØªÙˆÙ†ÛŒ Ø§Ø² Ú†ÛŒØ²ÛŒ Ú©Ù‡ Ø§Ø°ÛŒØªØª Ù…ÛŒâ€ŒÚ©Ù†Ù‡ Ø¨Ú¯ÛŒ..."
        ]
    return f"{np.random.choice(responses)} (Ø§Ø·Ù…ÛŒÙ†Ø§Ù†: {confidence:.2%})"

# Ú†Øª Ø¨Ø§ Ú©Ø§Ø±Ø¨Ø±
def interact_with_user():
    print("ğŸ‘‹ Ø³Ù„Ø§Ù…! Ù…Ù† ÛŒÙ‡ Ø±Ø¨Ø§Øª ØªØ­Ù„ÛŒÙ„ Ø§Ø­Ø³Ø§Ø³Ø§ØªÙ…!")
    print("Ù‡Ø± ÙˆÙ‚Øª Ø®ÙˆØ§Ø³ØªÛŒ ØªÙ…ÙˆÙ… Ú©Ù†ÛŒØŒ Ø¨Ù†ÙˆÛŒØ³ 'Ø®Ø±ÙˆØ¬'")

    with open("word_tokenizer.pkl", "rb") as f:
        tokenizer = pickle.load(f)
    model = load_model("emotion_analyzer.keras")

    user_inputs = []
    mood_scores = []

    while True:
        user_text = input("ğŸ’¬ Ø´Ù…Ø§: ").strip()
        if user_text.lower() in ["Ø®Ø±ÙˆØ¬", "exit", "quit"]:
            break

        user_inputs.append(user_text)
        cleaned_text = preprocess_text(user_text)
        seq = tokenizer.texts_to_sequences([cleaned_text])
        padded_seq = pad_sequences(seq, padding='post', maxlen=20)
        score = model.predict(padded_seq)[0][0]
        mood = "Ø®ÙˆØ´Ø­Ø§Ù„ ğŸ˜Š" if score > 0.5 else "Ù†Ø§Ø±Ø§Ø­Øª ğŸ˜"
        mood_scores.append(score)

        print(f"ğŸ˜ ØªØ­Ù„ÛŒÙ„: {mood}")
        print(f"ğŸ¤– Ù¾Ø§Ø³Ø®: {create_response(mood, score)}")

    if mood_scores:
        avg_mood = np.mean(mood_scores)
        overall_mood = "Ø®ÙˆØ´Ø­Ø§Ù„ ğŸ˜Š" if avg_mood > 0.5 else "Ù†Ø§Ø±Ø§Ø­Øª ğŸ˜"
        print("\nğŸ“ˆ ØªØ­Ù„ÛŒÙ„ Ù†Ù‡Ø§ÛŒÛŒ Ù…Ú©Ø§Ù„Ù…Ù‡:")
        print(f"ğŸ­ Ø­Ø³ Ú©Ù„ÛŒ Ø´Ù…Ø§: {overall_mood} (Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ø§Ù…ØªÛŒØ§Ø²: {avg_mood:.2%})")
    else:
        print("Ù‡ÛŒÚ† Ù…ØªÙ†ÛŒ ÙˆØ§Ø±Ø¯ Ù†Ø´Ø¯! ğŸ˜")

# Ø§Ø¬Ø±Ø§ÛŒ Ø¨Ø±Ù†Ø§Ù…Ù‡
if __name__ == "__main__":
    interact_with_user()